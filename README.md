# ğŸ… Tokyo Olympic Data Analytics Project

> *"Every expert was once a beginner. This project represents my journey into the world of big data analytics and cloud computing."*

## ğŸ“Š Project Overview

This project is my exploration into data engineering and analytics using real Tokyo 2020 Olympic data. As someone learning the ropes of big data technologies, I wanted to build something meaningful while getting hands-on experience with modern data stack tools.

### ğŸ¯ What I Set Out to Learn
- **Apache Spark & PySpark**: Big data processing and transformations
- **Azure Cloud Services**: Storage, compute, and data lake architecture
- **Data Pipeline Design**: ETL processes and data transformation workflows
- **Business Intelligence**: Creating insights from Olympic data

## ğŸ› ï¸ Tech Stack

| Technology | Purpose | Learning Experience |
|------------|---------|-------------------|
| **Apache Spark** | Data processing engine | *Discovered the power of distributed computing* |
| **PySpark** | Python API for Spark | *Learned functional programming concepts* |
| **Azure Data Lake Storage** | Cloud data storage | *First time working with cloud storage at scale* |
| **Azure Databricks** | Spark-based analytics platform | *Amazed by collaborative notebooks* |
| **Power BI** | Data visualization | *Created my first professional dashboard* |
| **Jupyter Notebooks** | Interactive development | *Perfect for experimentation and learning* |

## ğŸ“‚ Project Structure

```
ğŸ“ Olympic Data Analytics/
â”œâ”€â”€ ğŸ““ Tokyo Olympic Data Tranformation.ipynb  # Main Spark processing notebook
â”œâ”€â”€ ğŸ“Š dashboard.pbix                          # Power BI dashboard file
â”œâ”€â”€ ğŸ” AppRegistrations.txt                    # Azure service principal config
â”‚
â”œâ”€â”€ ğŸ“ data/                                   # Raw Olympic datasets
â”‚   â”œâ”€â”€ Athletes.csv        (11K+ records)     # Athlete information by country & discipline
â”‚   â”œâ”€â”€ Coaches.csv                            # Coaching staff data  
â”‚   â”œâ”€â”€ EntriesGender.csv                      # Gender distribution by discipline
â”‚   â”œâ”€â”€ Medals.csv          (94 countries)     # Medal tallies by country
â”‚   â””â”€â”€ Teams.csv                              # Team composition data
â”‚
â””â”€â”€ ğŸ“ transformed data/                       # Processed datasets
    â”œâ”€â”€ athletes.csv                           # Cleaned athlete data
    â”œâ”€â”€ coaches.csv                            # Processed coaching data
    â”œâ”€â”€ entriesgender.csv                      # Gender analysis ready data
    â”œâ”€â”€ medals.csv                             # Medal insights prepared
    â””â”€â”€ teams.csv                              # Team analysis dataset
```

## ğŸš€ What I Built

### 1. **Data Ingestion Pipeline**
- Connected to Azure Data Lake Storage using service principal authentication
- Mounted cloud storage to Databricks workspace
- Learned about secure credential management (the hard way! ğŸ˜…)

### 2. **Data Transformation Engine**
Using PySpark, I performed:
- **Schema corrections**: Fixed data types for numerical columns
- **Data exploration**: Analyzed athlete distributions and medal counts  
- **Analytics computations**: 
  - Calculated average gender participation by discipline
  - Ranked countries by gold medal performance
  - Identified disciplines with highest female participation

### 3. **ETL Pipeline**
- Automated data transformation workflows
- Implemented partitioned writes for better performance
- Created reusable data processing patterns

### 4. **Business Intelligence Dashboard**
- Built interactive Power BI dashboard
- Visualized Olympic performance metrics
- Connected cloud-transformed data to BI tools

## ğŸ“ Key Learning Moments

### ğŸ’¡ **Big Data Concepts I Grasped**
- **Distributed Computing**: Understanding how Spark processes data across clusters
- **Data Partitioning**: Learning why `.repartition(1)` was crucial for my output files
- **Lazy Evaluation**: Discovered how Spark optimizes query execution
- **Schema Evolution**: Handled data type mismatches in real datasets

### ğŸ”§ **Technical Skills Developed**
- **PySpark DataFrame API**: From basic operations to complex transformations
- **Azure Cloud Integration**: Service principals, storage accounts, and security
- **Data Pipeline Orchestration**: End-to-end workflow design
- **Error Handling**: Debugging distributed computing issues

### ğŸ“ˆ **Data Insights Discovered**
- **USA & China** dominated the medal tables (no surprise there!)
- **Gender representation** varies significantly across Olympic disciplines
- **Data quality issues** exist even in official Olympic datasets

## ğŸ¤” Honest Reflections

### âœ… **What Went Well**
- Successfully processed **11,000+ athlete records** using distributed computing
- Built my first cloud-based data pipeline from scratch
- Created meaningful analytics from real-world Olympic data
- Learned to debug Spark jobs and optimize performance

### ğŸ”„ **What I'd Improve Next Time**
- **Better error handling**: My initial attempts had hardcoded paths everywhere
- **Data validation**: Should have implemented schema validation earlier
- **Documentation**: Commenting my Spark transformations more thoroughly
- **Testing**: Creating unit tests for data transformation logic
- **Security**: Using Azure Key Vault instead of hardcoded credentials

### ğŸ¯ **Challenges I Overcame**
- **Authentication headaches**: Azure service principal setup took longer than expected
- **Schema mismatches**: Learning to handle inconsistent data types
- **Memory management**: Understanding Spark's lazy evaluation and caching strategies
- **Cloud costs**: Monitoring Azure resource usage (student budget! ğŸ’¸)

## ğŸ”® Next Steps

### ğŸŒŸ **Immediate Improvements**
- [ ] Implement proper secret management with Azure Key Vault
- [ ] Add data quality tests and validation rules
- [ ] Create automated data pipeline scheduling
- [ ] Expand analytics with more complex Olympic insights

### ğŸš€ **Future Enhancements**
- [ ] Real-time data streaming with Apache Kafka
- [ ] Machine learning models for Olympic performance prediction
- [ ] Advanced visualizations with D3.js or Tableau
- [ ] Containerized deployment with Docker

## ğŸ“š Resources That Helped Me

- **Apache Spark Documentation**: My constant companion
- **Azure Databricks Learning Path**: Excellent hands-on tutorials  
- **PySpark by Example**: Practical code snippets that saved me hours
- **Stack Overflow**: Where I found answers to my "why isn't this working?" questions

## ğŸ¤ Connect & Learn Together

This project represents my journey from data analytics beginner to someone who can wrangle big data in the cloud. If you're on a similar learning path, I'd love to connect and share experiences!

**What I learned most**: *Building data pipelines is like solving a puzzle - every piece needs to fit perfectly, and when it finally works, it's incredibly satisfying.*

---

*Built with curiosity, debugged with patience, and deployed with excitement! ğŸš€*

---

## ğŸ“Š Project Stats
- **Data Processed**: 11,000+ athlete records across 5 datasets
- **Countries Analyzed**: 94 participating nations  
- **Cloud Platform**: Microsoft Azure
- **Processing Engine**: Apache Spark 3.x
- **Development Time**: Learning curve included! â°
- **Coffee Consumed**: Immeasurable â˜•

*Remember: Every data engineer started with their first CSV file. This is mine! ğŸ“ˆ*
